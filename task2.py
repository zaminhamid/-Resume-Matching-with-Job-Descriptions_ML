# -*- coding: utf-8 -*-
"""task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RGqlIba64g5-HiwvBXB_M08R0AwPJnN
"""

from google.colab import files
uploaded = files.upload()
import io
import pandas as pd
df = pd.read_csv(io.BytesIO(uploaded['Resume.csv']))

!pip install PyPDF2 pdfminer.six pandas
import PyPDF2

!pip install transformers

import pandas as pd
import PyPDF2
from pathlib import Path

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file):
    pdf_text = ""
    with open(pdf_file, "rb") as file:
        pdf_reader = PyPDF2.PdfFileReader(file)
        for page_num in range(pdf_reader.numPages):
            page = pdf_reader.getPage(page_num)
            pdf_text += page.extractText()
    return pdf_text

# Create an empty list to store extracted text
extracted_texts = []

# Loop through each row of the DataFrame
for index, row in df.iterrows():
    pdf_file_path = row["Resume_str"]  # Adjust column name as needed
    pdf_path = Path(pdf_file_path)

    try:
        # Check if the path exists and is a file
        if pdf_path.is_file():
            pdf_text = extract_text_from_pdf(pdf_file_path)
            extracted_texts.append(pdf_text)
        else:
            # Handle cases where the path does not point to a valid file
            extracted_texts.append("")  # Add an empty string for non-existent files
    except Exception as e:
        # Handle any other exceptions that may occur during processing
        print(f"Error processing file at path {pdf_file_path}: {e}")

# Check if the lengths match before adding to the DataFrame
if len(extracted_texts) == len(df):
    # Add the extracted text as a new column in the DataFrame
    df["Extracted_Text"] = extracted_texts
else:
    print("Lengths do not match. Ensure DataFrame and PDF paths are correct.")

output_csv_filename = "Updated_Resume_With_Text.csv"

# Save the updated DataFrame to a new CSV file
df.to_csv(output_csv_filename, index=False)

# Print a message to confirm that the DataFrame has been saved
print(f"Updated DataFrame has been saved to {output_csv_filename}")

import pandas as pd

# Specify the filename of the saved CSV file
output_csv_filename = "Updated_Resume_With_Text.csv"

# Read the saved CSV file into a DataFrame
updated_df = pd.read_csv(output_csv_filename)

# Display the first few rows of the DataFrame
print(updated_df.head())

!pip install datasets
import PyPDF2
import re
import pandas as pd
from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.metrics.pairwise import cosine_similarity
import torch

# Load the pre-trained DistilBERT model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

# Function to extract text from PDF
def extract_text_from_pdf(pdf_file):
    pdf_text = ""
    with open(pdf_file, "rb") as file:
        pdf_reader = PyPDF2.PdfFileReader(file)
        for page_num in range(pdf_reader.numPages):
            page = pdf_reader.getPage(page_num)
            pdf_text += page.extractText()
    return pdf_text

# Load CVs from CSV
uploaded_cv = pd.read_csv("Updated_Resume_With_Text.csv")
job_descriptions = []

# Access a subset of job descriptions from Hugging Face (e.g., first 10)
# Access a subset of job descriptions from Hugging Face (e.g., first 10)
# Access a subset of job descriptions from Hugging Face (e.g., first 10)
from datasets import load_dataset
dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")
job_descriptions = [job_description for job_description in dataset[:10]]



# Initialize a dictionary to store the top 5 job matches for each CV
top_5_jobs_for_cv = {}

# Loop through each CV in the uploaded CSV
for cv_index, row in uploaded_cv.iterrows():
    cv_text = row["Resume_str"]
    cv_name = row["Category"]

    # Initialize a list to store the similarity scores for each job
    similarity_scores = []

    # Encode the CV
    cv_tokens = tokenizer(cv_text, padding=True, truncation=True, return_tensors="pt")

    with torch.no_grad():
        cv_embedding = model(**cv_tokens).last_hidden_state.mean(dim=1)

    # Calculate cosine similarity for each job description
    for job_desc_index, job_desc_text in enumerate(job_descriptions):
        # Encode the job description
        job_desc_tokens = tokenizer(job_desc_text, padding=True, truncation=True, return_tensors="pt")

        with torch.no_grad():
            job_desc_embedding = model(**job_desc_tokens).last_hidden_state.mean(dim=1)

        # Calculate cosine similarity
        similarity_score = cosine_similarity(cv_embedding, job_desc_embedding)[0][0]
        similarity_scores.append((job_desc_index, similarity_score))

    # Sort job descriptions by similarity score in descending order
    similarity_scores.sort(key=lambda x: x[1], reverse=True)

    # Get the top 5 job matches
    top_5_jobs = [(job_descriptions[job_desc_index], similarity_score) for job_desc_index, similarity_score in similarity_scores[:5]]

    # Store the top 5 job matches in the dictionary
    top_5_jobs_for_cv[cv_name] = top_5_jobs

# Print the top 5 job matches for each CV
for cv_name, top_5_jobs in top_5_jobs_for_cv.items():
    print(f"CV: {cv_name}")
    for i, (job_desc, similarity_score) in enumerate(top_5_jobs, start=1):
        print(f"Top {i} Job Match - Similarity Score: {similarity_score}")
        print(job_desc)
    print("\n")





